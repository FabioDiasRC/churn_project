{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849fee68",
   "metadata": {},
   "source": [
    "# Churn Prediction\n",
    "### Redes neurais para análise da saída de clientes\n",
    "O projeto abaixo visa o desenvolvimento de uma rede neural para identificar e auxiliar na previsao e análise da perda de clientes (churn) de uma base de dados economicos encontrados no Kaggle, [diretamente nesse link](https://www.kaggle.com/datasets/mervetorkan/churndataset).\n",
    "\n",
    "Nosso Conjunto de dados é baseado nos seguintes atributos:\n",
    "\n",
    "* <b> RowNumber:</b> número da linha (int);\n",
    "* <b> CustomerID:</b> número do Id (int);\n",
    "* <b> Surname:</b> Sobrenome da pessoa (string);\n",
    "* <b> CreditScore:</b> avaliação do crédito (int);\n",
    "* <b> Geography:</b> Região de origem (string);\n",
    "* <b> Gender:</b> genero (string);\n",
    "* <b> Age:</b> idade da pessoa (int);\n",
    "* <b> Tenure:</b> Quantidade de posses de valor que a pessoa tem (int);\n",
    "* <b> Balance:</b> balanço da conta (float64);\n",
    "* <b> NumOfPRoducts:</b> numero de produtos comprados (int);\n",
    "* <b> HasCrCard:</b> a pessoa possui ou nao cartao de credito (int - 0 não possui e 1 possui);\n",
    "* <b> IsActiveMember: </b> é um clinte ativo (int - 0 para sim, 1 para nao);\n",
    "* <b> Exilted:</b> se o cliente saiu ou nao da empresa (int - 0 para sim e 1 para nao).\n",
    "\n",
    "Agora iremos fazer as importações necessárias para o projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5429709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 11:43:43.438362: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-14 11:43:43.447088: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-14 11:43:43.470950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747233823.510755   26498 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747233823.523510   26498 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747233823.553756   26498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747233823.553785   26498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747233823.553790   26498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747233823.553793   26498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-14 11:43:43.562830: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d717b5",
   "metadata": {},
   "source": [
    "Carregando os dados e observando se eles estão carregados corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b5407c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados = pd.read_csv('src/dados/churn.csv')\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7752a4b",
   "metadata": {},
   "source": [
    "## Modificando os dados para treinar a rede\n",
    "Agora iremos observar os dados e efetuar as modificações necessárias dentro do conjunto para treinar a nossa rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c8d04e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dados.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87303d15",
   "metadata": {},
   "source": [
    "Como nossos dados vieram de um dataset limpo e conciso do Kaggle nosso dataset nao possui valores nulos e nem problemas que exigem tratamento aprofundado, uma vez que nosso objetivo é realmente a prática de desenvolvimento de uma rede neural.\n",
    "\n",
    "Uma observação inicial importante é sobre os atributos que temos nos dados do dataset, uma vez que as três primeiras colunas, <b>RowNumber, CustomerId e Surname não são necessárias na nossa análise e nao precisam fazer parte do nosso conjunto de dados </b>. Na verdade o CustomerId e o Surname podemos considerar dados senciveis (principalmente as duas ultimas se combinadas com algum outro conjunto de dados nao tratados) e ainda expor os clientes de um dataset real sem a necessidade, indo contra a lei LGPD. Entretanto nao iremos focar nesses dados de forma excessiva uma vez que sao dados liberados no kaggle e nosso objetivo é o estudo das redes neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c99308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure  Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2      0.0              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0          1               1        101348.88       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados = dados.drop(columns=['RowNumber','CustomerId', 'Surname'])\n",
    "dados.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f6a31",
   "metadata": {},
   "source": [
    "Agora que efetuamos a exclusao das nossas colunas desnecessárias podemos dar continuidade a nossa rede neural. Como observamos na descrição no topo do nosso arquivo o objetivo aqui é efetuar o modelo para descobrir e analisar a saida dos clientes dos dados que estamos analisando, coluna essa denominada como 'Exited', logo, essa coluna será o nosso atributo de resposta. Para separarmos os dados podemos então ja dividir nossos dados em X e y para começarmos a trabalhar os dados para a rede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e461cb5",
   "metadata": {},
   "source": [
    "### Padronizando os dados e dividindo em treino e teste\n",
    "\n",
    "Efetuaremos a separação do nosso atributo de objetivo, que será a saída ou nao do cliente em relação aos nossos dados. Para isso vamos trabalhar os dados que precisam de transformação categorica, sendo o  genero, que pode ser 'Male' (homem) ou 'Female' (mulher), e o 'Geograhphy' (que trás os paises do conjunto de dados), que podem ser 'France' (França), 'Spain' (Espanha) e 'Germany'(Alemanha).\n",
    "\n",
    "Para efetuar essa transformação iremos aplicar o 'columnTransformer' para aplicar a transformação em diferentes colunas e one_hot_enconding, pois esse comando vai transformar os valores em diferentes colunas e aplicar como zeros para negativos e um para positivos dos nosso dados. Por exemplo, existira uma coluna 'Male' para definir se a pessoa em questão é homem ou mulher, para homem sera um na coluna e para mulher sera zero, o mesmo ocorrendo na coluna 'Female', isso é util para trasnformar os dados que eram categoricos em dados que a rede neural entenda de forma efetiva e consiga aprender com os dados. Entao tem-se:\n",
    "* ColumnTransformer - permite a transformação em multiplas colunas do dataset;\n",
    "* transformers - a lista de transformações que iremos aplicar;\n",
    "* drop='first' - retira a primeira coluna após a transformação, que evita a multicolinearidade.\n",
    "* o remainder é aplicado para evitar a modificação das colunas que não foram citadas.\n",
    "\n",
    "Falando especificamente sobre a multicolinearidade, o 'drop='first' ajuda a evitar redundancia de informação, aumentando o desempenho do nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85de9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer(\n",
    "  transformers=[('onehoteconding', OneHotEncoder(drop='first'), ['Geography', 'Gender'])],\n",
    "                  remainder='passthrough')\n",
    "\n",
    "X = column_transformer.fit_transform(dados.drop(columns='Exited'))\n",
    "#aplicando a transformalçao para nosso X, onde será nossos dados com exceção do atributo target\n",
    "\n",
    "y = dados['Exited']\n",
    "#nosso atributo target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4bec0",
   "metadata": {},
   "source": [
    "Após a modificações dentro da nossa transformação aplicamos o 'columns_transformer' dentro dos nossos dados sendo os atributos passado para o valor de X, dados esses que serão os dados utilizados para treinar nossa rede neural. O dado de objetivo é o 'Exited', a saída da rede neural é a resposta da saída ou nao do cliente, entrando como valor y, que será nosso objetivo a ser encontrado.\n",
    "\n",
    "Agora iremos padronizar os numeros e na sequencia efetuar o treinamento. A padronização é importante uma vez que os dados apresenta valores desproporcionais entre eles no próprio atributo e entre os atributos, entao essa modificação permite um melhor desempenho do nosso modelo ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef8c54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5407cd",
   "metadata": {},
   "source": [
    "Efetuada a transformação podemos agora dividir nossos dados em treino e teste, que em resumo ira utilizar os dados de treino para treinar nossa rede neural e os dados de teste para efetuar os testes de acuracia da rede, sendo na proporção de 80% para teste e 20% para treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44bc1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598c008",
   "metadata": {},
   "source": [
    "## Sobre a rede a ser desenvolvida\n",
    "Alguns pontos relevantes sobre a rede neural que será desenvolvida:\n",
    "* Serão adicionadas camadas relacionadas com o Dense, onde cada camada intermediaria terá 16 neuronios, apenas um na camada de saida;\n",
    "* A função Relu para ativação também será aplicada nas camadas de inicio e do meio, buscando linearizar os valores entre zero e um;\n",
    "* A função de ativação na saída será sigmoide, uma vez que essa função é a mais indicada para uma resposta do tipo binaria;\n",
    "* Nosso termo de analise de perda será a 'binary_crossentropy', pois também é a mais indicada para casos binarios;\n",
    "* O otimizador é o Adam, para tentar diminuir as perdas;\n",
    "* A metrica de acerto sera a acuracia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28651ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabio/.local/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-05-14 11:43:51.718887: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6917 - loss: 0.6647\n",
      "Epoch 2/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7873 - loss: 0.5044\n",
      "Epoch 3/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8035 - loss: 0.4663\n",
      "Epoch 4/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8053 - loss: 0.4608\n",
      "Epoch 5/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8085 - loss: 0.4407\n",
      "Epoch 6/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8148 - loss: 0.4317\n",
      "Epoch 7/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8234 - loss: 0.4147\n",
      "Epoch 8/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8263 - loss: 0.4152\n",
      "Epoch 9/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8180 - loss: 0.4194\n",
      "Epoch 10/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4033\n",
      "Epoch 11/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8272 - loss: 0.4011\n",
      "Epoch 12/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.3966\n",
      "Epoch 13/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.3963\n",
      "Epoch 14/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8281 - loss: 0.3996\n",
      "Epoch 15/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.3941\n",
      "Epoch 16/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8368 - loss: 0.3962\n",
      "Epoch 17/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8425 - loss: 0.3752\n",
      "Epoch 18/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8451 - loss: 0.3840\n",
      "Epoch 19/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8428 - loss: 0.3773\n",
      "Epoch 20/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8475 - loss: 0.3669\n",
      "Epoch 21/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8511 - loss: 0.3570\n",
      "Epoch 22/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8437 - loss: 0.3781\n",
      "Epoch 23/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8378 - loss: 0.3879\n",
      "Epoch 24/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8522 - loss: 0.3645\n",
      "Epoch 25/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8465 - loss: 0.3746\n",
      "Epoch 26/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8419 - loss: 0.3773\n",
      "Epoch 27/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8385 - loss: 0.3905\n",
      "Epoch 28/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8472 - loss: 0.3742\n",
      "Epoch 29/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8488 - loss: 0.3652\n",
      "Epoch 30/30\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8503 - loss: 0.3678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x70c34a55f4a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "#adicionar as camadas\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=16, kernel_initializer='he_uniform', activation='relu', input_dim=X_train.shape[1]))\n",
    "classifier.add(Dropout(0.3))\n",
    "classifier.add(Dense(units=16, kernel_initializer='he_uniform', activation='relu'))\n",
    "classifier.add(Dropout(0.3))\n",
    "classifier.add(Dense(units=1, activation='sigmoid'))\n",
    "#ultima camada é apenas um neuronio, sendo sigmoid, por ser binario nossa resposta, a expectativa é uma melhor resposta\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(X_train, y_train, batch_size=10, epochs=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f0ad6",
   "metadata": {},
   "source": [
    "Nosso modelo conseguiu uma acuracia de 85% aproximadamente e uma perda de 0.36, o treinamento foi bem efetuado e conseguiu bons valores dentro do conjunto de dados. Importante ressalta que podemos ver a melhoria a cada nova rodada de treinamento e aprendizado dentro da nossa rede. Vale ressaltar que algumas das funções de ativaçoes e inicializadores merecem ser citadas de forma mais detalhada e serão comentados em um adendo ao fim desse arquivo.\n",
    "\n",
    "## Prevendo os dados\n",
    "Agora que temos nosso modelo treinado aplicamos o modelo nos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f140ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "['0.00%' '0.00%' '0.00%' ... '100.00%' '0.00%' '0.00%']\n"
     ]
    }
   ],
   "source": [
    "y_previsto = classifier.predict(X_test)\n",
    "y_previsto = (y_previsto >0.5)\n",
    "\n",
    "z_previsto = np.array([f\"{value[0] * 100:.2f}%\" for value in y_previsto])\n",
    "print(z_previsto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c778e2b",
   "metadata": {},
   "source": [
    "Agora aplicamos nossa confusion matrix para encontrar nossa acuracia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d253e3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1570</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>249</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1\n",
       "0  1570   37\n",
       "1   249  144"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = pd.DataFrame(confusion_matrix(y_test,y_previsto))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c6f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A acuracia da rede neural foi de 86.0%\n"
     ]
    }
   ],
   "source": [
    "total = y_previsto.shape[0]\n",
    "\n",
    "true_negatives = confusion_matrix.values[0][0]\n",
    "true_positives = confusion_matrix.values[1][1]\n",
    "\n",
    "acuracia = (true_negatives+true_positives)/total\n",
    "\n",
    "print(f'A acuracia da rede neural foi de {round(acuracia,2)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d97265",
   "metadata": {},
   "source": [
    "### Salvando o modelo\n",
    "Agora iremos salvar o modelo para finalizar o projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6a7827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "classifier.save('churn_classifier_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d4f6b",
   "metadata": {},
   "source": [
    "Caso queira importar o modelo, segue o codigo abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11e17d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from tensorflow.keras.models import load_model\\n\\n#Carrega o modelo salvo\\nmodelo_carregado = load_model('churn_classifier_model.h5')\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from tensorflow.keras.models import load_model\n",
    "\n",
    "#Carrega o modelo salvo\n",
    "modelo_carregado = load_model('churn_classifier_model.h5')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23f2d8",
   "metadata": {},
   "source": [
    "## Adendo sobre o Kernel_initializer e o Relu\n",
    "Como demonstrado o nosso modelo foi desenvolvido baseado em um problema de classificação binária, onde os valores podem assumir apenas a saida ou nao do cliente. Com aplicação de camadas Dense e com inicialização de pesos (kernel_initializer).\n",
    "\n",
    "No kernel_initializer utiliza o he_uniform para inicializar, com a aplicação do ReLu esse he_uniform evita a explosão do gradiente dentro da nossa rede, melhorando o desempenho do nosso modelo, iniciando já com uma distribuição uniforme de pesos.\n",
    "\n",
    "O ReLu (rectified Linear Unit) evita problemas com o gradiente (gradient vanishing) sendo muito bem aplicado em camadas ocultas, conseguindo o bom aprendizado do modelo de forma nao linear.\n",
    "\n",
    "O sigmoid é aplicada na camada final pois é indicado em saidas binarias do conjunto, sendo interpretado como uma porcentagem de acerto na saida, pois seus valores variam entre zero e um.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
